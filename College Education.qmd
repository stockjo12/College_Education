---
title: "College Education"
author: "Cooper Riggs & Stockton Nelson"
format: pdf
editor: visual
---

## Setup

```{r}
#Downloading Libraries
library(tidyverse)

#Bringing in Data
salary <- vroom::vroom("Salary.csv") |>
  janitor::clean_names() |>
  rename(major = major_category)
```

## Validation

### Linearity

```{r}
#Making General Linear Model
glm <- lm(salary ~ ., data = salary)

#Checking Linearity of GPA
car::avPlot(glm, "gpa",
            xlab = "GPA | Others",
            ylab = "Salary | Others",
            main = "Added-Variable Plot for GPA")
```

**Answer:** It looks like GPA has a nice, linear trend. There are no concerns here.

### Independence

**Answer:** We have no reason to believe that the salary of one person will affect the salary of another person.

### Normality

```{r}
stresids <- MASS::stdres(glm)
ggplot(data = data.frame(x = stresids), mapping = aes(x = x)) +
  geom_histogram(aes(y = after_stat(density))) +
  stat_function(fun = dnorm, color = "red", size = 1.5)
```

```{r}
#Running a KS-Test
ks.test(x = stresids, "pnorm", mean = 0, sd = 1)
```

**Answer:** There is no concern whatsoever with normality when looking at the histogram. We also are not concerned when running the KS Test.

### Equal Variance

```{r}
#Checking Equal Variance Assumption
ggplot(data = data.frame(fit = fitted(glm), res = stresids),
       aes(x = fit, y = res)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "blue") +
  labs(x = "Fitted",
       y = "Residuals")
```

```{r}
#Running a BP-Test
lmtest::bptest(glm)
```

**Answer:** Looking at the Fitted Vs Residuals Plot, we aren't concerned about equal variance being an issue. Looking at a BP test as well, we feel good about assuming equal variance.

## Problem 2 (Stockton)

### Impact of GPA on Salary

```{r}
#Running Linear Model with GPA 
lmgpa <- lm(salary ~ gpa, data = salary)

#Running Confidence Interval on GPA
confint(lmgpa, "gpa", level = 0.95)
```

```{r}
#Making Scatterplot of GPA and Salary
ggplot(data = salary, mapping = aes(x = gpa, y = salary)) +
  geom_point(color = "darkgreen", alpha = 0.8) +
  geom_smooth(method = lm, se = T, color = "forestgreen", fill = "palegreen") +
  scale_x_continuous(breaks = c(1.5, 2.0, 2.5, 3.0, 3.5, 4.0), 
                     labels = function(x) sprintf("%.1f", x)) +
  scale_y_continuous(labels = scales::dollar_format()) +
  labs(x = "GPA",
       y = "Salary",
       title = "The Effect of GPA on Salary",
       caption = "Figure 1: Relationship between GPA and Salary for college students.") +
  theme(plot.caption = element_text(hjust = 0))

ggsave("GPA_salary_plot.png", width = 8, height = 6, dpi = 300)
```

```{r}
#Calculating the Sample Size in Each Major
salary |>
  count(major)
```

**Answer:** A greater GPA has a positive impact on your salary. For every one point your GPA goes up, we expect your salary to go up somewhere between \$3,300 and \$6,300 dollars. While there is a positive impact on your salary, I do believe that GPA does positively increase your salary, I believe that this can mainly be explained by students with higher GPAs have a greater opportunity in what internships and graduate schools they are able to attend.

### Impact of GPA across Majors

```{r}
#Running Linear Model with GPA and Major Interaction
lmgm <- lm(salary ~ gpa * major, data = salary)

#Running Confidence Interval on GPA
fit <- lm(salary ~ gpa * major, data = salary)
confint(fit, level = 0.95)

salary |> 
  group_by(major) |> 
  summarize(n = n())
```

**Answer:** Looking at the 95% confidence interval for each of these interactions, it's safe to conclude that the effect for GPA is the same, regardless of your major. This could be due to the small sample sizes for each major, but with what data we got, all of these confidence intervals return intervals that contain 0, thus showing that there isn't an interaction.

## Problem 3 (Cooper)

### Differences in Salary Across Major

```{r}
# Fit linear model to evaluate effect of major on salary
# Model includes GPA and Sex as controls
sal_lm <- lm(Salary ~ GPA + Sex + MajorCategory, data = salary)

# Perform overall F-test to determine whether Major contributes significantly to predicting Salary
anova(sal_lm)
```

```{r}
# Load emmeans package to compute adjusted (marginal) means
library(emmeans)

# Compute adjusted mean salary for each major category
# GPA and Sex are held constant for fair comparison
major_means <- emmeans(sal_lm, ~ MajorCategory)

# Obtain 95% confidence intervals for adjusted means
confint(major_means)
```

```{r}
# Plot adjusted mean salaries with confidence intervals
# This visually compares expected salary across majors
plot(
  major_means,
  ylab = "Major Category",
  xlab = "Adjusted Mean Salary"
)
# Save plot for inclusion in article
ggsave("major_salary_plot.png", width = 8, height = 6, dpi = 300)
```

**Answers:**

Through running an F-test on the the linear model (Salary \~ GPA + Sex + Major) we get an extremely low p-value for major, suggesting that there is sufficient evidence to reject the null hypothesis. This means that there is a difference in salaries across majors. Through further inspection by taking the adjusted means of salaries for each major category we find that Engineering majors have a significantly higher salary. Other majors with higher salaries include Computers & Mathematics, Physical Sciences, Business, and Health.

## Problem 4 (Stockton)

### Differences in Salary Across Sex

```{r}
#Running Linear Model with Major and Sex Interaction
lmms <- lm(salary ~ gpa + major * sex, data = salary)

#Finding Marginal Means
emm <- emmeans::emmeans(lmms, ~ sex | major)

#Comparing Marginal Means Between Sex
confint(pairs(emm), level = 0.95)
```

```{r}
#Making a Graph of the Confidence Interval of the Differences
diffs <- pairs(emm) |> confint() |> as.data.frame() |>
  rename(
    Major = major,
    Difference = estimate,
    Lower = lower.CL,
    Upper = upper.CL
  ) |>
  mutate(Major = recode(Major, 
                        "Psychology & Social Work" = "Psychology",
                        "Agriculture & Natural Resources" = "Agriculture",
                        "Humanities & Liberal Arts" = "Humanities",
                        "Communications & Journalism" = "Communications",
                        "Computers & Mathematics" = "Mathematics",
                        "Biology & Life Science" = "Biology",
                        "Law & Public Policy" = "Law",
                        "Industrial Arts & Consumer Services" = "Industrial Arts"))
ggplot(diffs, aes(x = Difference, y = reorder(Major, Difference))) +
  geom_point(color = "#598BAF", size = 3) + 
  geom_errorbarh(aes(xmin = Lower, xmax = Upper), height = 0.3, color = "#598BAF") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "black") +
  scale_x_continuous(labels = scales::dollar_format()) +
  labs(
    x = "Difference in Salary",
    y = " ",
    title = "Gender Pay Gaps Across College Majors",
    subtitle = "Negative: Higher Earnings for Males   Positive: Higher Earnings for Females",
    caption = "Figure 2: Difference in Earnings between Males and Females among the same Major.") +
  theme(plot.caption = element_text(hjust = 0))

#Saving the Plot
ggsave("Gender_Pay_Gap.png", width = 8, height = 6, dpi = 300)
```

**Answers:** There is a difference between the earnings for males and females between college major. There are some majors where there is no difference, but there are also some majors where there's a stark difference and males earn most of the income.

## Problem 5 (Cooper)

### How Well do the Variables Explain Salary

```{r}
# Set seed for reproducibility so fold assignment is consistent
set.seed(123)

# Function to compute RMSE for a given fold
cross_validate <- function(fold_num) {
  
  # Split data into training and testing sets
  train_data <- salary[folds != fold_num, ]
  test_data  <- salary[folds == fold_num, ]
  
  # Fit linear model on training data only
  fit <- lm(
    salary ~ gpa + sex + major,
    data = train_data
  )
  # Generate predictions for test data
  preds <- predict(fit, newdata = test_data)
  
  # Compute RMSE for this fold
  rmse <- sqrt(mean((test_data$salary - preds)^2))
  
  return(rmse)
}

# Define number of folds for cross-validation
K <- 20

# Randomly assign each observation to one of K folds
folds <- rep(1:K, length = nrow(salary)) %>%
  sample()

# Apply cross-validation across all folds
rmse_results <- lapply(1:K, FUN = cross_validate) %>%
  unlist()
# Compute average RMSE across folds
mean(rmse_results)
```

```{r}
# Select one fold to demonstrate an out-of-sample prediction
fold_num <- 3

# Recreate training and testing split for that fold
train_data <- salary[folds != fold_num, ]
test_data  <- salary[folds == fold_num, ]

# Fit model using training data only
fit <- lm(Salary ~ GPA + Sex + MajorCategory, data = train_data)

# Select one observation from the test set
example <- test_data[1, ]

# Generate prediction for this observation
predicted_salary <- predict(fit, newdata = example)

# Extract actual salary and compute prediction error
actual_salary <- example$Salary
error <- actual_salary - predicted_salary

# Display actual value, predicted value, and error
print(c(actual_salary, predicted_salary, error))
```

**Answers:**

The variables in the data set do a pretty good job at predicting salary. After RMSE cross-validation, the mean RMSE across the folds is 5717.57. This means that on average, the model can predict a given person's salary with an error of about \$5,717.57. That's okay for predicting, but definitely not great. A couple of factors that are missing that we believe would also contribute to salary are degree level and location of the job. Adding these factors would likely lower the average RMSE. A specific example that shows how this model performs is if you had a female that majored in Communication and Journalism with a 2.79 GPA and a salary of \$84,200. The model would predict someone with these characteristics to have a salary of \$76,512.96, this is off by \$7,687.03. We would say that the difference between \$84,200 and \$76,512 in salary is fairly significant.
